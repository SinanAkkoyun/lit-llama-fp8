# Implementing TE FP8 inference speedup to LLMs
